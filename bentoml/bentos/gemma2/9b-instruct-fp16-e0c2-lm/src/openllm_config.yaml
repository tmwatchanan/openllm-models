engine_config:
  dtype: half
  max_model_len: 8192
  model: google/gemma-2-9b-it
extra_envs:
- name: VLLM_ATTENTION_BACKEND
  value: FLASHINFER
- name: HF_TOKEN
extra_labels:
  model_name: google/gemma-2-9b-it
  openllm_alias: 9b-lm,9b-instruct-lm
extra_requirements:
- --extra-index-url https://flashinfer.ai/whl/cu121/torch2.3
- flashinfer==0.1.2+cu121torch2.3
project: vllm-chat
service_config:
  name: gemma2
  resources:
    gpu: 1
    gpu_type: nvidia-tesla-l4
  traffic:
    timeout: 300
